import os
import torch
from torch import nn, optim
from torchvision import datasets, transforms, models
from torch.utils.data import DataLoader, Dataset
import matplotlib.pyplot as plt
from PIL import Image
from ignite.engine import Engine, Events
from ignite.handlers import EarlyStopping
from sklearn.model_selection import train_test_split

import kagglehub

# Download latest version
path = kagglehub.dataset_download("tawsifurrahman/covid19-radiography-database")

print("Path to dataset files:", path)

class Covid19RadiographyDataset(Dataset):
    def __init__(self, root_dir, transform=None):
        """
        Args:
            root_dir (str): Root directory of the dataset.
            transform (callable, optional): Optional transform to be applied to images.
        """
        self.root_dir = root_dir
        self.transform = transform
        self.image_paths = []
        self.labels = []
        self.classes = sorted(os.listdir(root_dir))  # Class names based on subfolders

        for label, subdir in enumerate(sorted(os.listdir(root_dir))):
            subdir_path = os.path.join(root_dir, subdir, "images")

            # Skip if not a valid directory
            if not os.path.isdir(subdir_path):
                continue

            # Append class and image paths
            self.classes.append(subdir)
            for file_name in os.listdir(subdir_path):
                self.image_paths.append(os.path.join(subdir_path, file_name))
                self.labels.append(label)

    def __len__(self):
        return len(self.image_paths)

    def __getitem__(self, idx):
        image_path = self.image_paths[idx]
        label = self.labels[idx]

        # Load image
        image = Image.open(image_path).convert("RGB")

        # Apply transformations
        if self.transform:
            image = self.transform(image)

        return image, label
    
    dataset_path = r"C:/Users/Admin/.cache/kagglehub/datasets/tawsifurrahman/covid19-radiography-database/versions/5/COVID-19_Radiography_Dataset"

# Define transformations
transform = transforms.Compose([
    transforms.Resize((224, 224)),  # Resize images to 224x224 (e.g., for ResNet)
    transforms.ToTensor(),          # Convert images to PyTorch tensors
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize (ImageNet stats)
])

# Create the dataset
dataset = Covid19RadiographyDataset(root_dir=dataset_path, transform=transform)

# Split into training and validation sets
from torch.utils.data import random_split


train_val_indices, test_indices = train_test_split(
    np.arange(len(dataset)), test_size=0.2, stratify=dataset.labels
)

train_indices, val_indices = train_test_split(
    train_val_indices,
    test_size=0.25,
    stratify=np.array(dataset.labels)[train_val_indices]
)

# Create subsets for train, validation, and test
train_subset = torch.utils.data.Subset(dataset, train_indices)
val_subset = torch.utils.data.Subset(dataset, val_indices)
test_subset = torch.utils.data.Subset(dataset, test_indices)

# Create DataLoaders
train_loader = DataLoader(train_subset, batch_size=32, shuffle=True)
val_loader = DataLoader(val_subset, batch_size=32, shuffle=False)
test_loader = DataLoader(test_subset, batch_size = 32, shuffle=False)

print("Classes:", dataset.classes)
print(f"Train size: {len(train_dataset)}, Validation size: {len(val_dataset)}")

dataset = Covid19RadiographyDataset(root_dir=dataset_path, transform=transform)

# Print sample information
print("Classes:", dataset.classes)
print(f"Total images: {len(dataset)}")

# Display first few images
from torch.utils.data import DataLoader
data_loader = DataLoader(dataset, batch_size=4, shuffle=True)

images, labels = next(iter(data_loader))
plt.figure(figsize=(10, 5))
for i in range(len(images)):
    plt.subplot(1, 4, i + 1)
    plt.imshow(images[i].permute(1, 2, 0))
    plt.title(dataset.classes[labels[i]])
    plt.axis("off")
plt.show()

# Load pre-trained VGG16 model
model_vgg = models.vgg16(pretrained=True)

# Modify the classifier to match the number of classes
num_classes = len(dataset.classes)
model_vgg.classifier[6] = nn.Linear(model_vgg.classifier[6].in_features, num_classes)

# Move the model to GPU if available
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model_vgg.to(device)

# Define loss function and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model_vgg.parameters(), lr=0.001)

from tqdm import tqdm

def train_model(model, train_loader, val_loader, criterion, optimizer, epochs=5, patience=2):
    best_val_loss = np.inf  # Initialize the best validation loss to infinity
    epochs_without_improvement = 0  # Counter for epochs without improvement

    for epoch in range(epochs):
        # Training phase
        model.train()
        running_loss = 0.0
        with tqdm(train_loader, unit="batch") as train_bar:  # Wrap the train_loader with tqdm
            for images, labels in train_bar:
                images, labels = images.to(device), labels.to(device)

                optimizer.zero_grad()
                outputs = model(images)
                loss = criterion(outputs, labels)
                loss.backward()
                optimizer.step()

                running_loss += loss.item()

                # Update the progress bar with loss
                train_bar.set_postfix(loss=running_loss/len(train_bar))

        # Validation phase
        model.eval()
        val_loss = 0.0
        correct = 0
        total = 0
        with torch.no_grad():
            with tqdm(val_loader, unit="batch") as val_bar:  # Wrap the val_loader with tqdm
                for images, labels in val_bar:
                    images, labels = images.to(device), labels.to(device)
                    outputs = model(images)
                    loss = criterion(outputs, labels)
                    val_loss += loss.item()

                    _, predicted = outputs.max(1)
                    correct += (predicted == labels).sum().item()
                    total += labels.size(0)

                    # Update the progress bar with validation loss
                    val_bar.set_postfix(val_loss=val_loss/len(val_bar), accuracy=100 * correct/total)

        avg_val_loss = val_loss / len(val_loader)
        print(f"Epoch {epoch+1}/{epochs}, "
              f"Train Loss: {running_loss/len(train_loader):.4f}, "
              f"Val Loss: {avg_val_loss:.4f}, "
              f"Accuracy: {100 * correct/total:.2f}%")

        # Check early stopping condition
        if avg_val_loss < best_val_loss:
            best_val_loss = avg_val_loss
            epochs_without_improvement = 0  # Reset the counter if validation loss improved
        else:
            epochs_without_improvement += 1
            if epochs_without_improvement >= patience:
                print(f"Early stopping: No improvement in validation loss for {patience} epochs.")
                break  # Stop training if no improvement in validation loss for 'patience' epochs


 # Train for 5 epochs
train_model(model_vgg, train_loader, val_loader, criterion, optimizer, epochs=15, patience=2)

def test_model(model, test_loader):
    model.eval()
    correct = 0
    total = 0
    with torch.no_grad():
        for images, labels in tqdm(test_loader, desc="Testing"):
            images, labels = images.to(device), labels.to(device)
            outputs = model(images)
            _, predicted = outputs.max(1)
            correct += (predicted == labels).sum().item()
            total += labels.size(0)
    accuracy = 100 * correct / total
    print(f"Test Accuracy: {accuracy:.2f}%")

# Test the model on the test set
test_model(model_vgg, test_loader)

